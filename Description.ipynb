{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled236.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXhuIS3JVAK5jTanicfeLY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lephuocdat2000/Tu-hu/blob/main/Description.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njpMhkMGE2JL",
        "outputId": "85f2e390-dfe4-4625-dd88-32c16a22c5b2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./content')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VksgmEMzDbL_"
      },
      "source": [
        "#Mô tả tập dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feoO61VRDfaQ"
      },
      "source": [
        "Tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn. Trong đó có 15,000 mẫu positive và 15,000 mẫu negative. Nguồn: https://streetcodevn.com/blog/dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBT31ObDt80"
      },
      "source": [
        "#Các bước huấn luyện"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAj_umCBD2kB"
      },
      "source": [
        "Có 5 bước chính để giải quyết bài toán phân tích cảm xúc trong văn bản:\n",
        "\n",
        "1) Huấn luyện một mô hình phát sinh ra vector từ (như mô hình Word2Vec) hoặc tải lên các vector từ tiền huấn luyện.\n",
        "\n",
        "2) Tạo ma trận ID cho tập dữ liệu huấn luyện\n",
        "\n",
        "\n",
        "3) Tạo mô hình RNN với các đơn vị LSTM, sử dụng tensorflow\n",
        "\n",
        "4) Huấn luyện mô hình RNN với dữ liệu ma trận đã tạo ở bước 2\n",
        "\n",
        "5) Đánh giá mô hình đã huấn luyện với tập test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nX2oXwcEKZl"
      },
      "source": [
        "##1>Load tập từ vựng và ma trận word embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMhhACpoEdpA"
      },
      "source": [
        "Đầu tiên, để có thể biến đổi một từ thành một vector, chúng ta sử dụng mô hình đã được huấn luyện trước đó (pretrained model). Mô hình đã train trước đó cho tiếng Việt được lấy ở đây: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
        "\n",
        "Tuy nhiên, số lượng từ vựng tiếng Việt được huấn luyện rất lớn, khoảng 2M từ. Mỗi từ được biểu diễn dưới dạng một vector 300 chiều. Với kích thước gốc của ma trận word embedding như vậy sẽ gây khó khăn cho việc load dữ liệu cũng như đưa vào thư viện tensorflow để huấn luyện nên chúng tôi đã tối giản lại với số lượng từ tối thiểu để có thể chạy được trên tập dữ liệu review về đồ ăn của Foody.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aF36ofYEczU",
        "outputId": "74fddf7e-3dca-4a39-94ac-a6ec75dd87d6"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#currentDir là mục chứa wordList.npy và wordVectors.npy\n",
        "currentDir = '/content/content/MyDrive/Deep Learning/Assignment3-SentimentAnalysis-with-LSTM'\n",
        "#file wordList.npy là file gồm những từ có trong 30000 câu tập huấn luyện về Foody  \n",
        "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
        "print('Simplified vocabulary loaded!')\n",
        "wordsList = wordsList.tolist()\n",
        "\n",
        "#wordVectors là tập ma trận embedding của những từ có trong wordsList được lấy ra từ tập pretrained Word2Vec ở trên\n",
        "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
        "wordVectors = np.float32(wordVectors)\n",
        "print ('Word embedding matrix loaded!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified vocabulary loaded!\n",
            "Word embedding matrix loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLgEeijgG6my"
      },
      "source": [
        "Để chắc chắn mọi dữ liệu được load lên một cách chính xác, chúng ta cần kiểm tra xem số lượng từ trong từ điển rút gọn và số chiều của ma trận word embedding có khớp với nhau hay không? Trong trường hợp này số từ mà chúng tôi giữ lại là 19,899 và số chiều trong không gian biểu diễn là 300 chiều."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nswm4VzaG9kK",
        "outputId": "3d9cea84-bd81-4fe2-c3b1-fdeb5d935c60"
      },
      "source": [
        "print('Size of the vocabulary: ', len(wordsList))\n",
        "print('Size of the word embedding matrix: ', wordVectors.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the vocabulary:  19899\n",
            "Size of the word embedding matrix:  (19899, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asIsYS3oHIAt"
      },
      "source": [
        "Ví dụ để hiểu rõ hơn: Giả sử ở trong wordsList từ \"ngon\" nằm ở index thứ i , để lấy được vector biểu diễn của từ \"ngon\" ta sẽ lấy \n",
        "wordVector[i] với shape = 1x300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX0Czs3IHzM5"
      },
      "source": [
        "##2>Khảo sát tập dữ liệu huấn luyện và tạo ma trận ID "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp0JOa4VH-r0"
      },
      "source": [
        "Tập dữ liệu lấy từ trang web Foody trên miền dữ liệu liên quan đến ẩm thực. Tập dữ liệu bao gôm 15.000 review tích cực đặt trong thư mục 'positiveReviews' và 15.000 review tiêu cực đặt trong thư mục 'negativeReviews'. Do khối lượng dữ liệu lớn, nếu chúng ta chọn số lượng từ tối đa (maxSeqLength) quá cao thì sẽ bị lãng phí khi biểu diễn ở những câu review quá ngắn. Ngược lại, nếu sử dụng số lượng từ tối đa quá ít thì sẽ bị bỏ lỡ những từ quan trọng giúp cho việc phân tích cảm xúc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZyX9IaoHACo"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "positiveFiles = [currentDir+'/positiveReviews/' + f for f in os.listdir(currentDir+ '/positiveReviews')]\n",
        "negativeFiles = [currentDir+'/negativeReviews/' + f for f in os.listdir(currentDir+ '/negativeReviews')]\n",
        "numWords = []\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)       \n",
        "print('Positive files finished')\n",
        "\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)  \n",
        "print('Negative files finished')\n",
        "\n",
        "numFiles = len(numWords)\n",
        "print('The total number of files is', numFiles)\n",
        "print('The total number of words in the files is', sum(numWords))\n",
        "print('The average number of words in the files is', sum(numWords)/len(numWords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQN_C2IbJokd"
      },
      "source": [
        "Chúng ta có thể sử dụng thư viện Matplot để minh hoạ phân bố về chiều dài của các câu review trong tập dữ liệu:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Day4MRIZVa"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(numWords, 50)\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axis([0, 1200, 0, 8000])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAPiSzkUJwoE"
      },
      "source": [
        "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy 180 sẽ là lựa chọn tương đối hợp lý."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66MrJlfuJ35A"
      },
      "source": [
        "maxSeqLength = 180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIb_kTNgKPEt"
      },
      "source": [
        "Để có cảm nhận rõ hơn về dữ liệu, chúng ta có thể hiển thị một số review bất kỳ như sau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzwOevrGKJfb"
      },
      "source": [
        "print('A positive sentence: ')\n",
        "fname = positiveFiles[3] # Randomly select a positive file to view\n",
        "with open(fname, encoding='utf-8') as f:\n",
        "    for lines in f:\n",
        "        print(lines)\n",
        "\n",
        "print('A negative sentence: ')\n",
        "fname = negativeFiles[10] # Randomly select a negative file to view\n",
        "with open(fname, encoding='utf-8') as f:\n",
        "    for lines in f:\n",
        "        print(lines)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NKlZqn1KTC7"
      },
      "source": [
        "##Chuẩn hóa văn bản và tách từ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PEYKH_WKylL"
      },
      "source": [
        "Tập dữ liệu đã được tách từ. Giữa hai từ có thể ghép lại để tạo thành một khái niệm mới chúng ta sử dụng ký tự '_' để nối các từ đó. Ví dụ: 'sinh_viên', 'sinh_học'.\n",
        "\n",
        "Sử dụng một hàm để xử lý và chuẩn hóa văn bản là 'cleanSentences'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtCqKwYcKUum"
      },
      "source": [
        "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ama7xiunLGBU"
      },
      "source": [
        "Bây giờ chúng ta sẽ biểu diễn 30.000 review dưới dạng các chỉ số của các từ. Tập dữ liệu positive và negative sẽ được tính hợp lại thành một ma trận 30000x180. Trong đó 30000 là số lượng review và 180 là số lượng từ tối đa cho một câu. Do bước chuẩn bị này tốn khá nhiều tài nguyên tính toán nên sau khi tính toán xong, chúng ta sẽ lưu lại để sử dụng cho những lần chạy thí nghiệm sau. Ma trận lưu trữ các chỉ số này là: 'ids'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIczY3FgLDYe"
      },
      "source": [
        "###Xác định chỉ số của từng từ trong review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW_MutCmLdtQ"
      },
      "source": [
        "Trong phần này chúng ta sẽ tiến hành tra cứu từng từ trong review, sau đó gán vào ma trận 'ids'. Trong đó chỉ số dòng của ma trận tương ứng với file review, chỉ số cột của ma trận tương ứng với một từ của review. Trường hợp từ nào không có trong tập từ điển thì ta sẽ gán bằng chỉ số của từ 'UNK' (unknow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcfG5zZLXWJ"
      },
      "source": [
        "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
        "nFiles = 0\n",
        "# Index of Unknow word\n",
        "unk_idx = wordsList.index('UNK')\n",
        "\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            # Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
        "            if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
        "            else: ids[nFiles][word_idx] = unk_idx\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "print('Positive files are indexed!')\n",
        "\n",
        "#Làm tương tự với file negative\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "             if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            else: ids[nFiles][word_idx] = unk_idx.\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "\n",
        "print('Negative files are indexed!')\n",
        "\n",
        "# Lưu lại ma trận ids để tiết kiệm thời gian\n",
        "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MU1RXQAMlRM"
      },
      "source": [
        "#Bước thực hiện trên tương đối mất thời gian.\n",
        "# Trường hợp đã tính toán và lưu ma trận 'ids' rồi thì ta có thể load lên để sử dụng luôn\n",
        "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
        "print('Word indexes of the first review: ', ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGVLDVVAOraM"
      },
      "source": [
        "#3> Xây dựng RNN với Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XFodNdeOzgX"
      },
      "source": [
        "Đầu tiên chúng tôi sẽ khởi tạo các tham số cho mô hình mạng RNN với các cell là các LSTM. Kiến trúc mạng ở đây bao gồm 128 đơn vị cho mỗi lớp, số lượng layer là 2, số lượng phân lớp là 2 và số vòng lặp khi huấn luyện là 30000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGxXmA-FO1bI"
      },
      "source": [
        "# Initialize paramters\n",
        "numDimensions = 300\n",
        "batchSize = 64\n",
        "lstmUnits = 128\n",
        "nLayers = 2\n",
        "numClasses = 2\n",
        "iterations = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov-OqBaPRk6N"
      },
      "source": [
        "Input của mạng là một câu đã được id hóa, output là một vector onehot với hai giá trị tương ứng với hai loại cảm xúc [1,0] positive và [0,1] negative "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh7w9hAiRF38"
      },
      "source": [
        "Chia tập dữ liệu với 2000 câu test(50 pos: 50 neg) và 28000 câu train(50 pos: 50 train) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFZJlSPlO5IB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = ids.copy()\n",
        "Y = np.concatenate((np.ones((15000,1)),np.zeros((15000,1))))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.067)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esTsjYLiQ9eL"
      },
      "source": [
        "Cấu trúc mô hình:\n",
        "- Với mỗi câu được id hóa đưa vào sẽ đi qua lớp Embedding và mỗi id đại diện cho một từ được chuyển hóa nhờ pretrained wordVectors thành vector 300 chiều.\n",
        "- Để tăng tính phức tạp cho mô hình ta sẽ xếp 2 lớp LSTM chồng lên nhau\n",
        "- Để tránh overfit ta sẽ xen giữa 2 lớp LSTM một lớp Dropout\n",
        "- Tiếp theo là một lớp fullyconnected \n",
        "- Cuối cùng là một lớp softmax để đưa ra kết quả là vector one-hot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l4fzEp2PX-4"
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "checkpoint_path = \"training_2/weights.{epoch:04d}.hdf5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=19899,output_dim=numDimensions,weights=[wordVectors],input_length=maxSeqLength,trainable=False),\n",
        "        layers.LSTM(lstmUnits,return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(lstmUnits),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(2,activation='softmax')]\n",
        " )\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='Adam',\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batchSize, epochs=iterations,callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji34ebZSVWvj"
      },
      "source": [
        "Ở epoch thứ 20 thì ta dừng lại vì accuracy trên tập test không tăng được nữa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmNl0P7oVRV1"
      },
      "source": [
        "Visualize lost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XODT1Zo_T9x7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = history.history['train_loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,20)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgEsjgFV922"
      },
      "source": [
        "Visualize accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84QW8ctMWA8Q"
      },
      "source": [
        "loss_train = history.history['acc']\n",
        "loss_val = history.history['val_acc']\n",
        "epochs = range(1,11)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASQoIGdoVws3"
      },
      "source": [
        "#4> Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBvhYjPiWFNO"
      },
      "source": [
        "Load weight đã train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6NidFyyVxvg"
      },
      "source": [
        "#weight_path là đường dẫn đến thư mục chứa file weight\n",
        "weight_path = 'training_1/weights.0020.hdf5'\n",
        "model.load_weights(weight_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvtxySEIWh3S"
      },
      "source": [
        "Hàm tiền xử lí"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4JycKVWWNS"
      },
      "source": [
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())\n",
        "\n",
        "def Predict_Sentence(sentence,model):\n",
        "   cleaned = cleanSentences(sentence)\n",
        "   words = cleaned.split()\n",
        "   word_vec = np.zeros((1,maxSeqLength))\n",
        "   for i,word in enumerate(words):\n",
        "      if word in wordsList:\n",
        "        word_idx = wordsList.index(word)\n",
        "      else:\n",
        "        word_idx = wordsList.index('UNK')\n",
        "      word_vec[0][i] = word_idx\n",
        "   result = model.predict(word_vec)\n",
        "   if np.argmax(result[0])==1: print('Positive')\n",
        "   else: print('Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUfl8O1WoS8"
      },
      "source": [
        "Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npP2rwKuWhC1"
      },
      "source": [
        "Predict_Sentence('Múp rụp',model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}